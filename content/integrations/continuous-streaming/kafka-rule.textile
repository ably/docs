---
title: Kafka rule
meta_description: "Ably allows you to send data to Kafka based on message, channel lifecycle, channel occupancy, and presence events."
meta_keywords: "Kafka, integrations, events, serverless"
languages:
  - none
redirect_from:
  - /general/firehose/kafka-rule
---

You can use a Kafka rule to send "data":/integrations/payloads#sources such as messages, occupancy, lifecycle and presence events from Ably to Kafka.

If you want to send data from Kafka to Ably, you can use the "Ably Kafka Connector":/general/kafka-connector, rather than Kafka rules.

h2(#creating-kafka-rule). Creating a Kafka rule

There are two ways to create a Kafka rule:

* Using the "Ably Dashboard":https://ably.com/dashboard.
* Using the "Control API":/account/control-api.

h3(#creating-rule-dashboard). Creating a Kafka rule in the dashboard

To create a rule in your "dashboard":https://ably.com/dashboard#58:

* Login and select the application you wish to integrate with Kafka.
* Click the *Integrations* tab.
* Click the *+ New Integration Rule* button.
* Choose Firehose.
* Choose Kafka.
* Configure the settings applicable to your use case and your Kafka installation.

h4(#general). Kafka general rule settings

The following explains the Kafka general rule settings:

|_. Section     |_. Purpose                                                                               |
|Source         |Select the type of event source.                                                         |
|Channel filter |Allows filtering of the rule based on a regular expression matching the channel name.    |
|Encoding       |Selects a setting for encoding the payload.                                              |
|Enveloped      |Wraps the payload with additional metadata when checked. Uncheck to receive raw payloads.|

h4(#specific). Kafka-specific settings

The following explains the Kafka-specific settings:

|_. Section       |_. Purpose                                                               |
|Routing key      |Used to route messages to Kafka topics.                                  |
|Mechanism        |Dropdown to select the SASL/SCRAM mechanism used for Kafka connection.   |
|Username         |Enter the username required to authenticate with the Kafka server.       |
|Password         |Enter the password required to authenticate with the Kafka server.       |
|Brokers          |List of Kafka broker endpoints in the format `<host>:<port>`.            |
|Another broker   |Option to add additional Kafka broker endpoints.                         |
|Create button    |Click to save and provision the Kafka settings for the integration rule. |

In this section you need to set up your Authentication for Kafka by selecting your preferred mechanism for authentication and providing credentials.

The supported Authentication Mechanisms are:

* "SASL/PLAIN":https://docs.confluent.io/platform/current/kafka/authentication_sasl/authentication_sasl_plain.html#kafka-sasl-auth-plain
* "SASL/SCRAM-SHA-256":https://docs.confluent.io/platform/current/kafka/authentication_sasl/authentication_sasl_scram.html#kafka-sasl-auth-scram
* "SASL/SCRAM-SHA-512":https://docs.confluent.io/platform/current/kafka/authentication_sasl/authentication_sasl_scram.html

* Click *Create* to create the rule.

h3(#creating-rule-control-api). Creating a Kafka rule using Control API

See some examples of creating rules in the "rules section":/account/control-api#examples-rules of the Control API documentation. You can also refer to the "rule section":/api/control-api#tag/rules/paths/~1apps~1{app_id}~1rules/post of the Control API Reference for information on creating a Kafka rule. You will need to select a @ruleType@ of @kafka@.
