---
title: Message batching
meta_description: "Send messages to multiple channels in a single transaction, or batch messages server-side before sending them to subscribers."
languages:
  - javascript
  - nodejs
  - ruby
  - python
  - php
  - java
  - swift
  - objc
  - csharp
  - flutter
  - go
---

Ably offers two different types of batching that have distinct use cases:

Server-side batching groups published messages into batches before sending them to subscribers. It is primarily intended for high-throughput scenarios where a large number of messages are being published to a channel in a short period of time. Each batch received by subscribers counts as a single message in terms of billing which can greatly reduce message costs in high-throughput scenarios.

Batch publishing enables clients to publish messages to multiple channels using a single request. It enables the same information to be easily distributed to multiple channels, or different information distributed to many channels, all using a single request. A similar feature is available to retrieve the "presence status of multiple channels":/docs/presence-occupancy/presence#batch using a single request.

h2(#server-side). Server-side batching

With server-side batching, Ably groups any messages published within a set period of time into batches, before sending them to subscribers.

It makes messages more cost efficient in high-throughput scenarios by reducing the number of messages being published, as each batch is counted as a single message. This also helps to mitigate the risk of hitting message rate limits at the same time. The interval over which batching occurs is configurable to ensure an appropriate trade off between cost efficiency and user experience, as a higher interval will increase the latency between message deliveries.

Server-side batching is effective in applications such as fan engagement platforms, where key moments in a game, such as points being scored, cause huge surges in the number of reactions and messages. It also enables a much higher number of users to be "present":/docs/presence-occupancy/presence on a channel when it is enabled.

<aside data-type='important'>
<p>Be aware that server-side batching doesn't support "idempotency":/docs/pub-sub/advanced#idempotency due to how messages are grouped on the server. However, if you are explicitly setting a message ID, then these messages will be excluded from being batched.</p>
</aside>

h3(#configure). Configure server-side batching

When configuring server-side batching, you need to configure a batching interval. This is the interval over which messages are batched into groups, in milliseconds. Messages sent to Ably during this interval are temporarily held and aggregated. Once the interval elapses, the collected messages are combined into a single batch and delivered to subscribers as one message.

Each batch can contain up to 200 messages by count or total data size. For example, if you have 210 messages, they will be split into two batches: one with 200 messages and another with 10 messages. If the combined data size of 200 messages exceeds the data limit, the excess will be allocated to a new batch as separate messages.

Use the following steps to configure server-side batching for a channel, or channel namespace:

# On your "dashboard":https://ably.com/accounts/any, select one of your apps.
# Go to *Settings*.
# Under "channel rules":/docs/channels#rules, click *Add new rule*.
# Enter the channel name, or channel namespace to apply server-side batching to.
# Check *Server-side batching enabled*.
# Choose a batching interval over which to aggregate messages.
# Click *Create channel rule* to save.

<aside data-type='note'>
<p>Server side batching is mutually exclusive with "message conflation":/docs/messages#conflation on a channel, or channel namespace.</p>
</aside>

h2(#batch-publish). Batch publish

It is possible to publish messages to multiple channels with a single request. A batch request queries an API multiple times with single HTTP request. A batch request has a single set of request details containing the request body, parameters and headers. These are converted into an array of requests to the underlying API. Each individual request to the underlying API is performed in parallel and may succeed or fail independently.

The following is an example of a batch publish request using the "@request()@":/docs/api/rest-sdk#request method to query the "batch REST API":/docs/api/rest-api#batch-publish

```[rest_javascript]
const ablyRest = new Ably.Rest({ key: '{{API_KEY}}' })
const content = { 'channels': [ 'test1', 'test2' ], 'messages': { 'data': 'myData' } }
const batchPublish = await ablyRest.request('post', '/messages', 2, null, content, null);

console.log('Success! status code was ' + batchPublish.statusCode)
```

```[rest_python]
ably_rest = AblyRest(key='{{API_KEY}}')

content = {
    "channels": ["test1", "test2"],
    "messages": {
        "data": 'myData'
    }
}

response = await ably_rest.request('POST', '/messages', body=content)

if response.is_success:
    print('Success! status code was', response.status_code)
else:
    print('An error occurred; err =', response.error_message)
```

```[rest_java]
ClientOptions options = new ClientOptions("{{API_KEY}}");
AblyRest ablyRest = new AblyRest(options);

JsonObject content = new JsonObject();
Gson gson = new Gson();

content.add("channels", gson.toJsonTree(new String[]{"test1", "test2"}));
JsonObject messages = new JsonObject();
messages.addProperty("data", "myData");
content.add("messages", messages);

final HttpCore.RequestBody body = HttpUtils.requestBodyFromGson(content, ablyRest.options.useBinaryProtocol);
HttpPaginatedResponse response = ablyRest.request("POST", "/messages", null, body, null);

System.out.println("Success! Status code was " + response.statusCode);
```

```[rest_php]
$rest = new Ably\AblyRest(
    ['key' => '{{API_KEY}}']
);
$content = ['channels' => ['test1', 'test2'], 'messages' => ['data' => 'myData']];
$batchPublish = $rest->request('POST', '/messages', [], $content);

echo('Success! status code was ' . $batchPublish->statusCode);
```

```[rest_go]
rest, err := ably.NewREST(
  ably.WithKey("{{API_KEY}}"),
)
if err != nil {
  log.Fatalf("Error creating Ably client: %v", err)
}

type Message struct {
  Data string `json:"data"`
}

type Content struct {
  Channels []string `json:"channels"`
  Messages Message  `json:"messages"`
}

// Create an instance of the Content structure
content := Content{
  Channels: []string{"test1", "test2"},
  Messages: Message{
    Data: "myData",
  },
}

response, err := rest.Request(
  "POST",
  "/messages",
  ably.RequestWithBody(content)).Pages(context.Background())

if err != nil {
  log.Fatalf("An error occurred; err = %v", err)
}
log.Printf("Success! status code was = %v", response.StatusCode())
```

h3(#batch-requests). Batch requests

Each batch publish request can contain a single @BatchSpec@ object, or an array of @BatchSpec@ objects. Each @BatchSpec@ object contains a single channel name or an array of channel names in the @channels@ property. The @messages@ property then contains a single message or an array of messages. Each @BatchSpec@ will then publish each of its messages to each of its channels.

For each channel, the messages grouped into a single @BatchSpec@ are published atomically. This means that:

* Either they will all be successfully published or none of them will
* The "max message size":/docs/pricing/limits#message limit applies to the total size of all messages in in a @BatchSpec@
* Each @BatchSpec@ will only count as a single message for the purpose of the "per-channel rate limit":/docs/pricing/limits#message

So if you do not need the atomicity guarantee and might be in danger of exceeding the max message size limit, you can put each message into its own @BatchSpec@ (relative ordering will still be preserved). Conversely, if you are publishing many hundreds of small messages and are in danger of exceeding the max per-channel message rate, you group them into a fewer @BatchSpecs@.

The batch request as a whole is subject to the following limits:

* Each request can only include 100 different channels. If the same channel name appears in multiple @BatchSpec@ objects within a single request, it only counts as one channel towards the 100 channel limit per batch request.
* Each request has a maximum body size of 2MiB.

The following is an example of a single @BatchSpec@ object publishing a single message to 2 channels:

```[text]
{
  channels: ['channel1', 'channel2'],
  messages: {data: 'My message contents'}
}
```

The following is an example of an array of @BatchSpec@ objects. The first publishes a single message to two channels and the second publishes two messages to a single channel:

```[text]
[
  {
    channels: ['channel1', 'channel2'],
    messages: {data: 'My message contents'}
  },
  {
    channels: 'channel3',
    messages: [
      {data: 'My message contents'},
      {name: 'an event', data: 'My event message contents'},
    ]
  }
]
```

The following is an example curl request, querying the "REST API":/docs/api/rest-api#batch-publish directly:

```[sh]
curl -X POST https://rest.ably.io/messages \
    -u "{{API_KEY}}" \
    -H "Content-Type: application/json" \
    --data '{ "channels": [ "test1", "test2"],
"messages": {"data": "My test message text" } }'
```

h3(#batch-responses). Batch responses

Once all requests have been completed in a batch request, a batch response is returned with three possible outcomes:

- Success := If all of the individual requests were successful then an array containing the response of each query is returned in request order.
- Failure := If the batch request itself failed before the individual requests were made, then an error response is returned with a status code and error response body. Examples of why the batch request can fail include an authorization failure or an invalid request.
- Partial success := If one or more of the individual requests failed the response body contains an error object with the error code @40020@ and a status code of @400@. The error body contains a @batchResponse@ array of each individual response in request order. The @batchResponse@ can be inspected if there is a need to know the details of each outcome. If you only need to know whether or not the batch request was completely successful then the status code is sufficient.

The examples for each possible outcome will use the following @BatchSpec@ object as the request data:

```[text]
{
  channels: ['channel0', 'channel1', 'channel2'],
  messages: {data: 'My test message text'}
}
```

The following is an example of a successful batch publish response. The response body contains the @messageId@ of each published message and the @channel@ it was published to. The status code is @201@:

```[json]
[
  {
    "channel":"channel0",
    "messageId":"w234r5t-fr5"
  },
  {
    "channel":"channel1",
    "messageId":"vde4sfc0p"
  },
  {
    "channel":"channel2",
    "messageId":"nh3exv8ih"
  }
]
```

The following is an example of a batch publish failure response. The response body contains the details of the @error@, in this example that the token used for the request has expired. The status code is @401@:

```[json]
{
  "error": {
    "message":"Token expired",
    "statusCode":401,
    "code":40140
  }
}
```

The following is an example of a batch publish partial success response. The successful requests contain the @messageId@ of each published message and the @channel@ they were published to. The failed request contains the @channel@ the request failed for and the details of the @error@, in this example that the credentials used didn't have the capability to publish to that channel. The status code for a partial success is always @400@:

```[json]
{
  "error": {
    "message": "Batched response includes errors",
    "statusCode":400,
    "code":40020
  }
  "batchResponse": [
    {
      "channel":"channel0",
      "messageId":"w234r5t-fr5"
    },
    {
      "channel":"channel1",
      "messageId":"vde4sfc0p"
    },
    {
      "channel":"channel2",
      "error": {
        "message": "Given credentials do not have the required capability",
        "statusCode": 401,
        "code": 40160
      }
    }
  ]
}
```
