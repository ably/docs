---
title: AI support chatbot
meta_description: "Calculate AI Transport pricing for conversations with an AI chatbot. Example shows how using the message-per-response pattern and modifying the append rollup window can generate cost savings."
meta_keywords: "chatbot, support chat, token streaming, token cost, AI Transport pricing, Ably AI Transport pricing, stream cost, Pub/Sub pricing, realtime data delivery, Ably Pub/Sub pricing"
intro: "This example uses consumption-based pricing for an AI support chatbot use case, where a single agent is publishing tokens to user over AI Transport."
---

### Assumptions

The scale and features used in this calculation.

### Cost summary

The high level cost breakdown for this scenario. Messages are billed for both inbound (published to Ably) and outbound (delivered to subscribers).

### Effect


- message-per-response Ably will automatically

- message-per-token you are in control, you can turn on server side batching to group messages together in a batching interval. Higher batching interval increases latency but reduces total number of messages, lower batching interval delivers messages quickly.
[server-side batching](/docs/messages/batch#server-side)

