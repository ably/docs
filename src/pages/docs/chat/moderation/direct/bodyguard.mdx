---
title: Bodyguard
meta_description: "Detect and remove unwanted content in a Chat Room using Bodyguard AI."
---

[Bodyguard](https://bodyguard.ai/) is a powerful contextual analysis platform that can be used to moderate content in chat rooms.

The Bodyguard rule is a rule applied to chat rooms in Ably Chat which enables you to use Bodyguard's content moderation capabilities to detect and handle inappropriate content before it is published to other users.

## Integration setup <a id="setup"/>

Configure the integration rule in your [Ably dashboard](https://ably.com/accounts/any/apps/any/integrations) or using the [Control API](/docs/account/control-api).

The following are the fields specific to Bodyguard configuration:

| Field | Description |
| ----- | ----------- |
| Bodyguard API key | The API key for your Bodyguard account. |
| Channel ID | The ID of your Bodyguard channel where moderation rules are configured. |
| Default Language (optional) | The default language to use for content analysis. This will be used as a fallback in case automatic language detection fails. |
| Model URL (optional) | A custom URL if using a custom moderation model. |

For additional configuration options shared across all before-publish moderation rules, see the [common configuration fields](/docs/chat/moderation#common-config).

Messages will be rejected if Bodyguard's analysis returns a `REMOVE` recommended action based on the moderation rules configured in your Bodyguard channel.

## Handling rejections <a id="rejections"/>

Messages are rejected when they fail Bodyguard's analysis. Bodyguard returns a REMOVE action in these instances and the messages will not be published to your channel. The publish request will be rejected. Moderation rejections will use the error code `42213`.
