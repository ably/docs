---
title: Message per token
meta_description: "Stream individual tokens from AI models as separate messages over Ably."
---

Token streaming with message-per-token is a pattern where every token generated by your model is published as its own Ably message. Each token then appears as one message in the channel history. This uses [Ably Pub/Sub](/docs/basics) for realtime communication between agents and clients.

This pattern is useful when clients only care about the most recent part of a response and you are happy to treat the channel history as a short sliding window rather than a full conversation log. For example:

- **Backend-stored responses**: The backend writes complete responses to a database and clients load those full responses from there, while Ably is used only to deliver live tokens for the current in-progress response.
- **Live transcription, captioning, or translation**: A viewer who joins a live stream only needs the last few tokens for the current "frame" of subtitles, not the entire transcript so far.
- **Code assistance in an editor**: Streamed tokens become part of the file on disk as they are accepted, so past tokens do not need to be replayed from Ably.
- **Autocomplete**: A fresh response is streamed for each change a user makes to a document, with only the latest suggestion being relevant.

## Publishing tokens <a id="publishing"/>

Publish tokens from a [Realtime](/docs/api/realtime-sdk) client, which maintains a persistent connection to the Ably service. This allows you to publish at very high message rates with the lowest possible latencies, while preserving guarantees around message delivery order. For more information, see [Realtime and REST](/docs/basics#realtime-and-rest).

[Channels](/docs/channels) separate message traffic into different topics. For token streaming, each conversation or session typically has its own channel.

Use the [`get()`](/docs/api/realtime-sdk/channels#get) method to create or retrieve a channel instance:

<Code>
```javascript
const channel = realtime.channels.get('{{RANDOM_CHANNEL_NAME}}');
```
</Code>

When publishing tokens, don't await the `channel.publish()` call. Ably rolls up acknowledgments and debounces them for efficiency, which means awaiting each publish would unnecessarily slow down your token stream. Messages are still published in the order that `publish()` is called, so delivery order is not affected.

<Code>
```javascript
// ✅ Do this - publish without await for maximum throughput
for await (const event of stream) {
  if (event.type === 'token') {
    channel.publish('token', event.text);
  }
}

// ❌ Don't do this - awaiting each publish reduces throughput
for await (const event of stream) {
  if (event.type === 'token') {
    await channel.publish('token', event.text);
  }
}
```
</Code>

This approach maximizes throughput while maintaining ordering guarantees, allowing you to stream tokens as fast as your AI model generates them.

## Streaming patterns <a id="patterns"/>

Ably is a pub/sub messaging platform, so you can structure your messages however works best for your application. Below are common patterns for streaming tokens, each showing both agent-side publishing and client-side subscription. Choose the approach that fits your use case, or create your own variation.

### Continuous token stream <a id="continuous"/>

For simple streaming scenarios such as live transcription, where all tokens are part of a continuous stream, simply publish each token as a message.

#### Publish tokens

<Code>
```javascript
const channel = realtime.channels.get('{{RANDOM_CHANNEL_NAME}}');

// Example: stream returns events like { type: 'token', text: 'Hello' }
for await (const event of stream) {
  if (event.type === 'token') {
    channel.publish('token', event.text);
  }
}
```
</Code>

#### Subscribe to tokens

<Code>
```javascript
const channel = realtime.channels.get('{{RANDOM_CHANNEL_NAME}}');

// Subscribe to token messages
await channel.subscribe('token', (message) => {
  const token = message.data;
  console.log(token); // log each token as it arrives
});
```
</Code>

This pattern is simple and works well when you're displaying a single, continuous stream of tokens.

### Token stream with multiple responses <a id="multiple-responses"/>

For applications with multiple responses, such as chat conversations, include a `responseId` in message [extras](/docs/messages#properties) to correlate tokens together that belong to the same response.

#### Publish tokens

<Code>
```javascript
const channel = realtime.channels.get('{{RANDOM_CHANNEL_NAME}}');

// Example: stream returns events like { type: 'token', text: 'Hello', responseId: 'resp_abc123' }
for await (const event of stream) {
  if (event.type === 'token') {
    channel.publish({
      name: 'token',
      data: event.text,
      extras: {
        headers: {
          responseId: event.responseId
        }
      }
    });
  }
}
```
</Code>

#### Subscribe to tokens

Use the `responseId` header in message extras to correlate tokens. The `responseId` allows you to group tokens belonging to the same response and correctly handle token delivery for multiple responses, even when delivered concurrently.

<Code>
```javascript
const channel = realtime.channels.get('{{RANDOM_CHANNEL_NAME}}');

// Track responses by ID
const responses = new Map();

await channel.subscribe('token', (message) => {
  const token = message.data;
  const responseId = message.extras?.headers?.responseId;

  if (!responseId) {
    console.warn('Token missing responseId');
    return;
  }

  // Create an empty response
  if (!responses.has(responseId)) {
    responses.set(responseId, '');
  }

  // Append token to response
  responses.set(responseId, responses.get(responseId) + token);
});
```
</Code>

### Token stream with explicit start/stop events <a id="explicit-events"/>

In some cases, your AI model response stream may include explicit events to mark response boundaries. You can indicate the event type, such as a response start/stop event, using the Ably message name.

#### Publish tokens

<Code>
```javascript
const channel = realtime.channels.get('{{RANDOM_CHANNEL_NAME}}');

// Example: stream returns events like:
// { type: 'message_start', responseId: 'resp_abc123' }
// { type: 'message_delta', responseId: 'resp_abc123', text: 'Hello' }
// { type: 'message_stop', responseId: 'resp_abc123' }

for await (const event of stream) {
  if (event.type === 'message_start') {
    // Publish response start
    channel.publish({
      name: 'start',
      extras: {
        headers: {
          responseId: event.responseId
        }
      }
    });
  } else if (event.type === 'message_delta') {
    // Publish tokens
    channel.publish({
      name: 'token',
      data: event.text,
      extras: {
        headers: {
          responseId: event.responseId
        }
      }
    });
  } else if (event.type === 'message_stop') {
    // Publish response stop
    channel.publish({
      name: 'stop',
      extras: {
        headers: {
          responseId: event.responseId
        }
      }
    });
  }
}
```
</Code>

#### Subscribe to tokens

Handle each event type to manage response lifecycle:

<Code>
```javascript
const channel = realtime.channels.get('{{RANDOM_CHANNEL_NAME}}');

const responses = new Map();

// Handle response start
await channel.subscribe('start', (message) => {
  const responseId = message.extras?.headers?.responseId;
  responses.set(responseId, '');
});

// Handle tokens
await channel.subscribe('token', (message) => {
  const responseId = message.extras?.headers?.responseId;
  const token = message.data;

  const currentText = responses.get(responseId) || '';
  responses.set(responseId, currentText + token);
});

// Handle response stop
await channel.subscribe('stop', (message) => {
  const responseId = message.extras?.headers?.responseId;
  const finalText = responses.get(responseId);
  console.log('Response complete:', finalText);
});
```
</Code>
